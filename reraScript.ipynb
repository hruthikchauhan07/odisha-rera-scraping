{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1f46883-4cd7-4995-9db1-30eec4defd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in e:\\anacondaaa\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in e:\\anacondaaa\\lib\\site-packages (4.12.3)\n",
      "Collecting selenium\n",
      "  Using cached selenium-4.33.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting webdriver-manager\n",
      "  Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\anacondaaa\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anacondaaa\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\anacondaaa\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anacondaaa\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in e:\\anacondaaa\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting trio~=0.30.0 (from selenium)\n",
      "  Using cached trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.12.2 (from selenium)\n",
      "  Using cached trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting typing_extensions~=4.13.2 (from selenium)\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in e:\\anacondaaa\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: python-dotenv in e:\\anacondaaa\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in e:\\anacondaaa\\lib\\site-packages (from webdriver-manager) (24.1)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.30.0->selenium)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: sortedcontainers in e:\\anacondaaa\\lib\\site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
      "Collecting outcome (from trio~=0.30.0->selenium)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in e:\\anacondaaa\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in e:\\anacondaaa\\lib\\site-packages (from trio~=0.30.0->selenium) (1.17.1)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in e:\\anacondaaa\\lib\\site-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in e:\\anacondaaa\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in e:\\anacondaaa\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.14.0)\n",
      "Using cached selenium-4.33.0-py3-none-any.whl (9.4 MB)\n",
      "Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Using cached trio-0.30.0-py3-none-any.whl (499 kB)\n",
      "Using cached trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: wsproto, urllib3, typing_extensions, certifi, attrs, outcome, webdriver-manager, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.8.30\n",
      "    Uninstalling certifi-2024.8.30:\n",
      "      Successfully uninstalled certifi-2024.8.30\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "Successfully installed attrs-25.3.0 certifi-2025.4.26 outcome-1.3.0.post0 selenium-4.33.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.13.2 urllib3-2.4.0 webdriver-manager-4.0.2 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88a5d249-e098-45c1-990b-0f70ae58c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "740ae1d0-7b6b-4983-82bc-deb44981216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OdishaRERAScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://rera.odisha.gov.in\"\n",
    "        self.projects_url = \"https://rera.odisha.gov.in/projects/project-list\"\n",
    "        self.session = requests.Session()\n",
    "        self.driver = None\n",
    "        \n",
    "    def setup_driver(self):\n",
    "        \"\"\"Setup Chrome WebDriver with options\"\"\"\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")  # Run in background\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        \n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        self.driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        return self.driver\n",
    "    \n",
    "    def get_project_list(self):\n",
    "        \"\"\"Get the list of first 6 projects from the main page\"\"\"\n",
    "        try:\n",
    "            self.setup_driver()\n",
    "            self.driver.get(self.projects_url)\n",
    "            \n",
    "            # Wait for the page to load\n",
    "            wait = WebDriverWait(self.driver, 20)\n",
    "            \n",
    "            # Look for project table or list\n",
    "            try:\n",
    "                # Wait for table to load (adjust selector based on actual HTML structure)\n",
    "                table = wait.until(EC.presence_of_element_located((By.TAG_NAME, \"table\")))\n",
    "                \n",
    "                # Find all \"View Details\" links or project rows\n",
    "                view_details_links = self.driver.find_elements(By.XPATH, \"//a[contains(text(), 'View Details')]\")\n",
    "                \n",
    "                projects = []\n",
    "                for i, link in enumerate(view_details_links[:6]):  # Get first 6 projects\n",
    "                    try:\n",
    "                        project_url = link.get_attribute('href')\n",
    "                        # Get RERA registration number from the same row\n",
    "                        row = link.find_element(By.XPATH, \"./ancestor::tr\")\n",
    "                        cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                        \n",
    "                        rera_no = cells[1].text.strip() if len(cells) > 1 else \"N/A\"\n",
    "                        project_name = cells[2].text.strip() if len(cells) > 2 else \"N/A\"\n",
    "                        \n",
    "                        projects.append({\n",
    "                            'rera_no': rera_no,\n",
    "                            'project_name': project_name,\n",
    "                            'detail_url': project_url\n",
    "                        })\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error extracting project {i+1}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                return projects\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error finding project table: {str(e)}\")\n",
    "                # Try alternative approach - look for any links containing project details\n",
    "                page_source = self.driver.page_source\n",
    "                soup = BeautifulSoup(page_source, 'html.parser')\n",
    "                \n",
    "                # Look for project links (adjust based on actual HTML structure)\n",
    "                project_links = soup.find_all('a', href=True)\n",
    "                projects = []\n",
    "                \n",
    "                for link in project_links[:6]:\n",
    "                    if 'project' in link.get('href', '').lower():\n",
    "                        projects.append({\n",
    "                            'rera_no': 'N/A',\n",
    "                            'project_name': link.text.strip(),\n",
    "                            'detail_url': self.base_url + link['href'] if not link['href'].startswith('http') else link['href']\n",
    "                        })\n",
    "                \n",
    "                return projects\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting project list: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def get_project_details(self, project_url):\n",
    "        \"\"\"Extract detailed information from project detail page\"\"\"\n",
    "        try:\n",
    "            self.driver.get(project_url)\n",
    "            wait = WebDriverWait(self.driver, 15)\n",
    "            \n",
    "            # Wait for page to load\n",
    "            time.sleep(3)\n",
    "            \n",
    "            details = {}\n",
    "            \n",
    "            # Try to find and click \"Promoter Details\" tab\n",
    "            try:\n",
    "                promoter_tab = self.driver.find_element(By.XPATH, \"//a[contains(text(), 'Promoter Details') or contains(text(), 'Promoter')]\")\n",
    "                promoter_tab.click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                print(\"Promoter Details tab not found, continuing...\")\n",
    "            \n",
    "            # Get page source and parse with BeautifulSoup\n",
    "            page_source = self.driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            \n",
    "            # Extract promoter company name\n",
    "            company_elements = soup.find_all(text=lambda text: text and 'company' in text.lower())\n",
    "            details['promoter_name'] = \"N/A\"\n",
    "            for elem in company_elements:\n",
    "                parent = elem.parent\n",
    "                if parent and parent.next_sibling:\n",
    "                    details['promoter_name'] = parent.next_sibling.strip()\n",
    "                    break\n",
    "            \n",
    "            # Extract registered office address\n",
    "            address_keywords = ['registered office', 'address', 'office address']\n",
    "            details['promoter_address'] = \"N/A\"\n",
    "            \n",
    "            for keyword in address_keywords:\n",
    "                address_elements = soup.find_all(text=lambda text: text and keyword in text.lower())\n",
    "                for elem in address_elements:\n",
    "                    parent = elem.parent\n",
    "                    if parent and parent.next_sibling:\n",
    "                        details['promoter_address'] = parent.next_sibling.strip()\n",
    "                        break\n",
    "                if details['promoter_address'] != \"N/A\":\n",
    "                    break\n",
    "            \n",
    "            # Extract GST number\n",
    "            gst_elements = soup.find_all(text=lambda text: text and 'gst' in text.lower())\n",
    "            details['gst_no'] = \"N/A\"\n",
    "            for elem in gst_elements:\n",
    "                parent = elem.parent\n",
    "                if parent and parent.next_sibling:\n",
    "                    gst_text = parent.next_sibling.strip()\n",
    "                    if len(gst_text) == 15 and gst_text.isalnum():  # GST format check\n",
    "                        details['gst_no'] = gst_text\n",
    "                        break\n",
    "            \n",
    "            # Alternative approach: look for table data\n",
    "            tables = soup.find_all('table')\n",
    "            for table in tables:\n",
    "                rows = table.find_all('tr')\n",
    "                for row in rows:\n",
    "                    cells = row.find_all(['td', 'th'])\n",
    "                    if len(cells) >= 2:\n",
    "                        label = cells[0].text.strip().lower()\n",
    "                        value = cells[1].text.strip()\n",
    "                        \n",
    "                        if 'company' in label or 'promoter' in label:\n",
    "                            details['promoter_name'] = value\n",
    "                        elif 'address' in label and 'office' in label:\n",
    "                            details['promoter_address'] = value\n",
    "                        elif 'gst' in label:\n",
    "                            details['gst_no'] = value\n",
    "            \n",
    "            return details\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting project details from {project_url}: {str(e)}\")\n",
    "            return {\n",
    "                'promoter_name': 'N/A',\n",
    "                'promoter_address': 'N/A',\n",
    "                'gst_no': 'N/A'\n",
    "            }\n",
    "    \n",
    "    def scrape_projects(self):\n",
    "        \"\"\"Main method to scrape all project data\"\"\"\n",
    "        try:\n",
    "            print(\"Starting to scrape Odisha RERA projects...\")\n",
    "            \n",
    "            # Get list of projects\n",
    "            projects = self.get_project_list()\n",
    "            print(f\"Found {len(projects)} projects\")\n",
    "            \n",
    "            if not projects:\n",
    "                print(\"No projects found. The website structure might have changed.\")\n",
    "                return []\n",
    "            \n",
    "            # Get detailed information for each project\n",
    "            detailed_projects = []\n",
    "            \n",
    "            for i, project in enumerate(projects, 1):\n",
    "                print(f\"Processing project {i}/{len(projects)}: {project['project_name']}\")\n",
    "                \n",
    "                details = self.get_project_details(project['detail_url'])\n",
    "                \n",
    "                complete_project = {\n",
    "                    'rera_regd_no': project['rera_no'],\n",
    "                    'project_name': project['project_name'],\n",
    "                    'promoter_name': details['promoter_name'],\n",
    "                    'promoter_address': details['promoter_address'],\n",
    "                    'gst_no': details['gst_no']\n",
    "                }\n",
    "                \n",
    "                detailed_projects.append(complete_project)\n",
    "                \n",
    "                # Add delay to be respectful to the server\n",
    "                time.sleep(2)\n",
    "            \n",
    "            return detailed_projects\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in scraping process: {str(e)}\")\n",
    "            return []\n",
    "        \n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56a0fc1-05b1-4ef7-9ca5-48ae4b74a7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrape Odisha RERA projects...\n",
      "Error finding project table: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x003CFC03+61635]\n",
      "\tGetHandleVerifier [0x003CFC44+61700]\n",
      "\t(No symbol) [0x001F05D3]\n",
      "\t(No symbol) [0x0023899E]\n",
      "\t(No symbol) [0x00238D3B]\n",
      "\t(No symbol) [0x00280E12]\n",
      "\t(No symbol) [0x0025D2E4]\n",
      "\t(No symbol) [0x0027E61B]\n",
      "\t(No symbol) [0x0025D096]\n",
      "\t(No symbol) [0x0022C840]\n",
      "\t(No symbol) [0x0022D6A4]\n",
      "\tGetHandleVerifier [0x00654523+2701795]\n",
      "\tGetHandleVerifier [0x0064FCA6+2683238]\n",
      "\tGetHandleVerifier [0x0066A9EE+2793134]\n",
      "\tGetHandleVerifier [0x003E68C5+155013]\n",
      "\tGetHandleVerifier [0x003ECFAD+181357]\n",
      "\tGetHandleVerifier [0x003D7458+92440]\n",
      "\tGetHandleVerifier [0x003D7600+92864]\n",
      "\tGetHandleVerifier [0x003C1FF0+5296]\n",
      "\tBaseThreadInitThunk [0x758F7BA9+25]\n",
      "\tRtlInitializeExceptionChain [0x7767C0CB+107]\n",
      "\tRtlClearBits [0x7767C04F+191]\n",
      "\n",
      "Found 0 projects\n",
      "No projects found. The website structure might have changed.\n"
     ]
    }
   ],
   "source": [
    "scraper = OdishaRERAScraper()\n",
    "projects = scraper.scrape_projects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c40e468-7b3d-4859-b79b-75294e23dd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No projects were scraped successfully.\n"
     ]
    }
   ],
   "source": [
    "if projects:\n",
    "    print(f\"\\nSuccessfully scraped {len(projects)} projects:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, project in enumerate(projects, 1):\n",
    "        print(f\"{i}. RERA No: {project['rera_regd_no']}\")\n",
    "        print(f\"   Project Name: {project['project_name']}\")\n",
    "        print(f\"   Promoter Name: {project['promoter_name']}\")\n",
    "        print(f\"   Promoter Address: {project['promoter_address']}\")\n",
    "        print(f\"   GST No: {project['gst_no']}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Convert to DataFrame for better display\n",
    "    df = pd.DataFrame(projects)\n",
    "    display(df)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv('rera_projects_jupyter.csv', index=False)\n",
    "    print(\"\\nData saved to 'rera_projects_jupyter.csv'\")\n",
    "else:\n",
    "    print(\"No projects were scraped successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6995f5-a67a-46ce-9a4b-490f558b7342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
